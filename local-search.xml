<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>flow matching</title>
    <link href="/2025/07/07/flow-matching/"/>
    <url>/2025/07/07/flow-matching/</url>
    
    <content type="html"><![CDATA[<h3 id="概率密度函数的变量变换">概率密度函数的变量变换</h3><p>给定随机变量 <span class="math inline"><em>z</em> ∼ <em>π</em>(<em>z</em>)</span>，通过映射函数 <span class="math inline"><em>f</em></span>得到新的随机变量 <span class="math inline"><em>x</em> = <em>f</em>(<em>z</em>)</span>，若存在逆函数 <span class="math inline"><em>f</em><sup>−1</sup></span>，则新变量 <span class="math inline"><em>x</em></span> 的概率密度函数<span class="math inline"><em>p</em>(<em>x</em>)</span> 为： * <span class="math inline"><em>z</em></span> 为随机变量 <span class="math display">$$p(x)=\pi(z)\left|\frac{dz}{dx}\right|=\pi\left(f^{-1}(x)\right)\left|\frac{df^{-1}}{dx}\right|=\pi\left(f^{-1}(x)\right)\left|\left(f^{-1}\right)^{\prime}(x)\right|$$</span> * <span class="math inline"><em>z</em></span> 为随机向量 <span class="math display">$$p(\mathbf{x})=\pi(\mathbf{z})\left|\det\frac{d\mathbf{z}}{d\mathbf{x}}\right|=\pi\left(f^{-1}(\mathbf{x})\right)\left|\det\frac{df^{-1}}{d\mathbf{x}}\right|$$</span></p><h3 id="归一化流-normalizing-flows">归一化流 （ Normalizing Flows）</h3><p><strong>Normalizing Flows(NFs)</strong>是一种可逆的概率密度变换方法，它的核心思想是通过一系列可逆的变换函数来逐步将一个简单分布（通常是高斯分布）转换成一个复杂的目标分布。这个过程可以被看作是一连串的变量替换的迭代过程，每次替换都遵循概率密度函数的变量变换原则。原始分布<span class="math inline"><em>p</em><sub>0</sub>(<em>z</em><sub>0</sub>)</span>，通过一系列的变换<span class="math inline">{<em>f</em><sub><em>i</em></sub>}</span>将其转换成目标分布 <span class="math inline"><em>p</em><sub>1</sub>(<em>x</em>)</span>，变换过程可以表示为： <span class="math display"><strong>x</strong> = <strong>z</strong><sub><em>K</em></sub> = <em>f</em><sub><em>K</em></sub> ∘ <em>f</em><sub><em>K</em> − 1</sub> ∘ ⋯ ∘ <em>f</em><sub>1</sub>(<strong>z</strong><sub>0</sub>)</span>对于第 <span class="math inline"><em>i</em></span> 步，有： <span class="math display"><strong>z</strong><sub><em>i</em> − 1</sub> ∼ <em>p</em><sub><em>i</em> − 1</sub>(<strong>z</strong><sub><em>i</em> − 1</sub>)</span><span class="math display"><strong>z</strong><sub><em>i</em></sub> = <em>f</em><sub><em>i</em></sub>(<strong>z</strong><sub><em>i</em> − 1</sub>), thus<strong>z</strong><sub><em>i</em> − 1</sub> = <em>f</em><sub><em>i</em></sub><sup>−1</sup>(<strong>z</strong><sub><em>i</em></sub>)</span>代入概率密度变量变换关系有： <span class="math display">$$p_i\left(\mathbf{z}_i\right)=p_{i-1}\left(f_i^{-1}\left(\mathbf{z}_i\right)\right)\left|\det\frac{df_i^{-1}}{d\mathbf{z}_i}\right|=p_{i-1}\left(\mathbf{z}_{i-1}\right)\left|\det\frac{df_i}{d\mathbf{z}_{i-1}}\right|^{-1}$$</span>对数似然： <span class="math display">$$\logp_i\left(\mathbf{z}_i\right)=\logp_{i-1}\left(\mathbf{z}_{i-1}\right)-\log\left|\det\frac{df_i}{d\mathbf{z}_{i-1}}\right|$$</span>递推至初始分布： <span class="math display">$$\logp(\mathbf{x})=\log\pi_K\left(\mathbf{z}_K\right)=\log\pi_{K-1}\left(\mathbf{z}_{K-1}\right)-\log\left|\det\frac{df_K}{d\mathbf{z}_{K-1}}\right|=\log\pi_0\left(\mathbf{z}_0\right)-\sum_{i=1}^K\log|\det\frac{df_i}{d\mathbf{z}_{i-1}}|$$</span>当一系列变换函数 <span class="math inline"><em>f</em><sub><em>i</em></sub></span>可逆，优化目标为负对数似然： <span class="math display">$$\mathcal{L}(\mathcal{D})=-\frac{1}{|\mathcal{D}|}\sum_{\mathbf{x}\in\mathcal{D}}\logp(\mathbf{x})$$</span> ### 连续归一化流（ Continuous Normalizing Flows）<strong>Continuous Normalizing Flows(CNFs)</strong> 是 <strong>NormalizingFlows</strong> 的一种扩展，它可以更好地建模复杂的概率分布。在传统的归一化流中，变换通常是通过一系列<strong>可逆的离散函数</strong>来定义的，而在<strong>CNFs</strong>中，这种变换是<strong>连续的</strong>，这使得模型能够更加平滑地适应数据的分布，提高了模型的表达能力。<strong>CNFs</strong>过程通过<strong>常微分方程（ODE）</strong> 来表示： <span class="math display">$$\frac{d}{dt}x_{t}=v_{t}(x_{t})$$</span> 其中 <span class="math inline"><em>v</em><sub><em>t</em></sub>(<em>x</em><sub><em>t</em></sub>)</span>是时间和状态相关的<strong>速度场</strong>，这个速度场决定了数据如何从<span class="math inline"><em>p</em><sub>0</sub></span> 转换为 <span class="math inline"><em>p</em><sub>1</sub></span>，通常使用神经网络预测，<span class="math inline"><em>x</em><sub><em>t</em></sub></span>是<strong>FowMap</strong> ，简单理解为时间 <span class="math inline"><em>t</em></span>下的数据点。理论上知道了速度场就可以通过求解微分方程推出任意一个噪声点是如何移动的，从而得到整个分布的变换过程，但是在训练时，需要一个目标训练场来指导模型的学习。</p><p><strong>概率流连续性方程</strong>：由于概率密度的性质确保了在全体分布上的积分为1，这说明概率的总和是固定的，即概率也是守恒的，类似于流体力学中的连续性方程，数据的概率密度分布<span class="math inline"><em>p</em><sub><em>t</em></sub></span>随时间变化也满足连续性方程： <span class="math display">$$\frac{\partial{p_{t}(x)}}{\partial{t}}=-\nabla\cdot[v_{t}(x)p_{t}(x)]$$</span> 连续化方程描述了概率密度 <span class="math inline"><em>p</em><sub><em>t</em></sub>(<em>x</em><sub><em>t</em></sub>)</span>随时间的演化关系，其中 <span class="math inline">∇⋅</span>是散度算子，表示某个向量场的发散程度，<span class="math inline"><em>p</em><sub><em>t</em></sub>(<em>x</em>)</span>表示在 <span class="math inline"><em>t</em></span> 时刻点 <span class="math inline"><em>x</em></span> 处的概率密度，<span class="math inline"><em>v</em><sub><em>t</em></sub>(<em>x</em>)</span>是时间 <span class="math inline"><em>t</em></span>处的速度场，表示数据点如何随时间移动。 ### 条件概率流 对于目标分布为<span class="math inline"><em>p</em><sub>1</sub>(<em>x</em><sub>1</sub>)</span>的每个数据样本 <span class="math inline"><em>x</em><sub>1</sub></span>，将目标数据点本身作为条件变量，定义随时间变化的条件概率路径<span class="math inline"><em>p</em><sub><em>t</em></sub>(<em>x</em>|<em>x</em><sub>1</sub>)</span>，对条件概率进行边缘化积分，得到边缘概率路径：<span class="math display"><em>p</em><sub><em>t</em></sub>(<em>x</em>) = ∫<em>p</em><sub><em>t</em></sub>(<em>x</em>, <em>x</em><sub>1</sub>)<em>d</em><em>x</em><sub>1</sub> = ∫<em>p</em><sub><em>t</em></sub>(<em>x</em> ∣ <em>x</em><sub>1</sub>)<em>p</em><sub>1</sub>(<em>x</em><sub>1</sub>)<em>d</em><em>x</em><sub>1</sub></span>条件向量场：<span class="math inline"><strong>u</strong><sub><strong>t</strong></sub>(<em>x</em>|<em>x</em><sub><em>t</em></sub>)</span>，表示给定 <span class="math inline"><em>x</em><sub>1</sub></span>的条件下，当前时刻 <span class="math inline"><em>t</em></span>的速度向量。 边缘向量场：<span class="math inline"><strong>u</strong><sub><strong>t</strong></sub>(<em>x</em>)</span>，表示总体概率质量如何在时间上流动，是所求的目标，通过对每个条件向量场加权积分后得到边缘向量场。<span class="math display">$$\mathbf{u}_t(x)=\int\mathbf{u}_t(x\midx_1)\cdot\frac{p_t(x\mid x_1)p_{1}(x_1)}{p_t(x)}dx_1$$</span> 权重：<span class="math display">$$\frac{p_t(x\mid x_1)p_{1}(x_1)}{p_t(x)}$$</span> 表示边缘密度中 <span class="math inline"><em>x</em><sub>1</sub></span> 的条件概率的占比 ###流匹配 对于连续归一化流，最直观的方法就是在给定初始条件 <span class="math inline"><em>x</em><sub>0</sub></span> 的条件下，通过ODE求解来得到 <span class="math inline"><em>x</em><sub>1</sub><sup>′</sup></span>的分布，然后通过一种最小化差异的度量（kl散度等）来约束预测值 <span class="math inline"><em>x</em><sub>1</sub><sup>′</sup></span> 和真实值<span class="math inline"><em>x</em><sub>1</sub></span>的分布保持一致。然而反复<strong>模拟ODE</strong>的计算量巨大，因此这种方法可实施性不高。流匹配通过确保模型预测的向量场与描述数据点实际运动的向量场之间的动态特性保持一致性，从而确保通过CNFs变换得到的最终概率分布与期望的目标分布相匹配。直接的训练目标是使用神经网络<span class="math inline"><em>V</em><sub><em>θ</em></sub>(<em>x</em>, <em>t</em>)</span>匹配理想的边缘向量场 <span class="math inline"><em>u</em><sub><em>t</em></sub>(<em>x</em>)</span>。<span class="math display">ℒ<sub>marginal</sub>(<em>θ</em>) = 𝔼<sub><strong>x</strong> ∼ <em>p</em><sub><em>t</em></sub>(<strong>x</strong>)</sub>||<strong>v</strong><sub><em>θ</em></sub>(<strong>x</strong>, <em>t</em>) − <strong>u</strong><sub><em>t</em></sub>(<strong>x</strong>)||<sup>2</sup></span>流匹配的核心思想是最小化这个损失函数，使得学习的向量场 <span class="math inline"><strong>v</strong><sub><em>θ</em></sub>(<strong>x</strong>, <em>t</em>)</span>和真实向量场 <span class="math inline"><em>u</em><sub><em>t</em></sub>(<em>x</em>)</span>尽可能接近，从而能确保能准确生成目标概率密度路径 <span class="math inline"><em>p</em><sub><em>t</em></sub>(<em>x</em>)</span>。 但往往缺乏先验知识来确定合适的 <span class="math inline"><em>u</em><sub><em>t</em></sub>(<em>x</em>)</span>和 <span class="math inline"><em>p</em><sub><em>t</em></sub>(<em>x</em>)</span>，因此其无法直接使用，为了解决这个问题，论文提出了<strong>条件流匹配（Conditional FlowMatching）</strong>，可以采用一种基于样本的方法，实现向量场和概率路径的求解。通过为每个样本独立定义概率路径和向量场，之后将其聚合得到整个概率路径和向量场。论文证明了优化CFM的目标等同于优化FM的目标，无需计算边缘概率路径或边缘向量场，<strong>只需要设计一个合适的条件概率路径和向量场即可。</strong></p><h3 id="矫正流">矫正流</h3><p>使用线性插值定义概率路径： <span class="math display"><em>x</em><sub><em>t</em></sub> = (1 − <em>t</em>) ⋅ <em>x</em><sub>0</sub> + <em>x</em><sub>1</sub> ⋅ <em>t</em></span>对时间求导：得到目标向量场： <span class="math display"><em>u</em><sub><em>t</em></sub>(<em>x</em><sub><em>t</em></sub>|<em>x</em><sub>0</sub>, <em>x</em><sub>1</sub>) = <em>x</em><sub>1</sub> − <em>x</em><sub>0</sub></span>条件流匹配（CFM）的损失可以定义为： <span class="math display">ℒ<sub>CFM</sub>(<em>θ</em>) = 𝔼<sub><em>t</em>, <strong>x</strong><sub>0</sub>, <strong>x</strong><sub>1</sub></sub>[||<strong>v</strong><sub><em>θ</em></sub>(<strong>x</strong><sub><em>t</em></sub>, <em>t</em>) − <strong>u</strong><sub><em>t</em></sub>(<strong>x</strong><sub><em>t</em></sub>|<strong>x</strong><sub>0</sub>, <strong>x</strong><sub>1</sub>)||<sup>2</sup>]</span>其中，<span class="math inline">𝔼</span> 表示对时间 <span class="math inline"><em>t</em> ∼ <em>U</em>(0, 1)</span> ，噪声 <span class="math inline"><em>x</em><sub>0</sub> ∼ <em>p</em><sub>0</sub></span>和目标分布 <span class="math inline"><em>x</em><sub>1</sub> ∼ <em>p</em><sub>1</sub></span>进行采样求期望，实际计算形式为： <span class="math display">ℒ<sub>RectifiedFlow</sub>(<em>θ</em>) = 𝔼<sub><em>t</em>, <strong>x</strong><sub>0</sub>, <strong>x</strong><sub>1</sub></sub>[||<strong>v</strong><sub><em>θ</em></sub>((1 − <em>t</em>)<strong>x</strong><sub>0</sub> + <em>t</em><strong>x</strong><sub>1</sub>, <em>t</em>) − (<strong>x</strong><sub>1</sub> − <strong>x</strong><sub>0</sub>)||<sup>2</sup>]</span>数学上可以严格证明： <strong>最小化条件匹配流损失</strong> <span class="math inline">ℒ<sub>CFM</sub></span> 等价于最小化边缘匹配流损失<span class="math inline">ℒ<sub>marginal</sub></span> 故： <span class="math display">∇<sub><em>θ</em></sub>ℒ<sub>CFM</sub>(<em>θ</em>) = ∇<sub><em>θ</em></sub>ℒ<sub>marginal</sub>(<em>θ</em>)</span>通过让模型学习 <span class="math inline"><em>u</em><sub><em>t</em></sub>(<em>x</em><sub><em>t</em></sub>|<em>x</em><sub>0</sub>, <em>x</em><sub>1</sub>)</span>，即可得到目标向量场。### 算法流程 对于训练中的每一步： 1. 从均匀分布 <span class="math inline"><em>U</em>(0, 1)</span> 中随机采样一个时间 <span class="math inline"><em>t</em></span> 2. 采样数据 <span class="math inline"><em>x</em><sub>1</sub> ∼ <em>p</em><sub>1</sub></span>，<span class="math inline"><em>x</em><sub>0</sub> ∼ <em>p</em><sub>0</sub></span>3. 构造训练对： * 输入样本：<span class="math inline"><em>x</em><sub><em>t</em></sub> = (1 − <em>t</em>) ⋅ <em>x</em><sub>0</sub> + <em>x</em><sub>1</sub> ⋅ <em>t</em></span>* 目标向量：<span class="math inline"><em>u</em> = <em>x</em><sub>1</sub> − <em>x</em><sub>0</sub></span>4. 将 <span class="math inline"><em>x</em><sub><em>t</em></sub>、<em>t</em></span>输入到神经网络 <span class="math inline"><strong>v</strong><sub><strong>θ</strong></sub></span>得到预测向量 <span class="math inline">$\hat{\mathbf{u}} =\mathbf{v}_\theta(x_t,t)$</span> 5. 计算损失，<span class="math inline">$L=||\hat{\mathbf{u}}-\mathbf{u}||$</span> 6.更新参数 ### [DDPM] 与 FM流匹配的核心是学习一个向量场，之后使用数值方法求解，向量场确定后演化路径唯一确定，采样结果唯一确定扩散模型理论上基于随机微分方程，在采样时会额外添加随机的偏移，最终生成的结果会有细微的差别。</p>]]></content>
    
    
    <categories>
      
      <category>生成式模型</category>
      
    </categories>
    
    
    <tags>
      
      <tag>生成式、深度学习、概率模型</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Diffusion</title>
    <link href="/2025/07/06/Diffusion/"/>
    <url>/2025/07/06/Diffusion/</url>
    
    <content type="html"><![CDATA[<h1 id="ddpm">DDPM</h1><ol type="1"><li>加噪过程一步到位，去噪过程严格按照一步一步去噪，实现对噪声的预测，由于去噪过程需要一步一步进行，所以每次都要调用神经网络，导致生成较慢，所以考虑DDIM等加速生成的算法框架</li><li>加噪过程：前向过程被认为是马尔可夫过程，需要将数据分布逐步转换为噪声分布<span class="math display">$$q(x_0,\ x_1,\ \dots\ ,x_T)=q(x_0)\prod^{T}_{i}q(x_t|x_{t-1})$$</span> 输入数据<span class="math inline"><em>x</em></span>，噪声水平<span class="math inline"><em>t</em></span>，真实噪声<span class="math inline"><em>ϵ</em></span>。</li><li>逆向过程：从噪声分布逆向还原数据分布，学习目标： <span class="math display">$$p_{\theta}(x_0,x_1,\dots,x_T)=p(x_T)\prod^{T}_{i}p_{\theta}(x_{t-1}|x_t)$$</span></li></ol><h2 id="数学推导">数学推导</h2><p>扩散模型的训练本质上是变分推断，通过最大化证据下界（ELBO）来近似真实的后验分布<span class="math inline"> <em>p</em>(<em>x</em><sub><em>t</em> − 1</sub>|<em>x</em><sub><em>t</em></sub>)</span>，<span class="math inline"><em>q</em>(<em>x</em><sub><em>t</em> − 1</sub>|<em>x</em><sub><em>t</em></sub>, <em>x</em><sub>0</sub>) </span>是真实后验的一个变分近似，而<span class="math inline"> <em>p</em><sub><em>θ</em></sub>(<em>x</em><sub><em>t</em> − 1</sub>|<em>x</em><sub><em>t</em></sub>)</span>是模型学习的分布</p><p>原始数据点分布<span class="math inline"><em>x</em><sub>0</sub> ∼ <em>q</em>(<em>x</em><sub>0</sub>)</span>，构造马尔可夫加噪过程：<span class="math display">$$q(x_{1:T}|x_0)=\prod^{T}_{i}q(x_t|x_{t-1})$$</span> 状态转移核服从高斯条件分布： <span class="math display">$$q(x_t|x_{t-1})=\mathcal{N}(x_t\ ; \, \sqrt{1-\beta_t}x_{t-1}\ ,\ \beta\mathbf{I})\ ,\ \beta_t \in (0,1)_{t=1}^T$$</span> <span class="math inline"><em>β</em><sub><em>t</em></sub></span>为预定的噪声方差系数，由时间<span class="math inline"><strong>1</strong> <strong>∼</strong> <strong>T</strong></span>逐渐递增，可以是线性、指数等，<span class="math inline">$\sqrt{1-\beta_t}x_{t-1}\,$</span>为高斯分布的均值<em>经推导</em>： <span class="math display">$$\begin{equation}x_t = \sqrt{\alpha_t} x_0 + \sqrt{1 - {\alpha}_t} \, \epsilon, \quad\epsilon \sim \mathcal{N}(0,1)\end{equation}$$</span> 任意步的数值都可以从<span class="math inline"><em>x</em><sub>0</sub></span>导出，其中<span class="math inline">$\,\alpha_t=\prod^{t}_{s=1}(1-\beta_s)\,$</span></p><p>逆向过程也是一个马尔可夫链，通过学习条件概率<span class="math inline"> <em>p</em><sub><em>θ</em></sub>(<em>x</em><sub><em>t</em> − 1</sub>|<em>x</em><sub><em>t</em></sub>) </span>逐步恢复<span class="math inline"><em>x</em><sub>0</sub></span>，引入神经网络<span class="math inline"><em>p</em><sub><em>θ</em></sub></span>来近似 <span class="math display"> <em>p</em><sub><em>θ</em></sub>(<em>x</em><sub><em>t</em> − 1</sub>|<em>x</em><sub><em>t</em></sub>)  = 𝒩(<em>x</em><sub><em>t</em> − 1</sub>; <em>μ</em><sub><em>θ</em></sub>(<em>x</em><sub><em>t</em></sub>, <em>t</em>), ∑<sub><em>θ</em></sub>(<em>x</em><sub><em>t</em></sub>, <em>t</em>))</span>其中，<span class="math inline"><em>μ</em><sub><em>θ</em></sub></span>和 <span class="math inline">∑<sub><em>θ</em></sub></span>是神经网络需要学习的均值和方差，通常<span class="math inline"> <em>p</em><sub><em>θ</em></sub>(<em>x</em><sub><em>t</em> − 1</sub>|<em>x</em><sub><em>t</em></sub>) </span>难以训练，可以使用<span class="math inline"><em>q</em>(<em>x</em><sub><em>t</em> − 1</sub>|<em>x</em><sub><em>t</em></sub>) </span>近似，而<span class="math inline"><em>q</em>(<em>x</em><sub><em>t</em> − 1</sub>|<em>x</em><sub><em>t</em></sub>) </span>难以直接估计，无法直接求出闭式解，所以使用可计算的真实后验<span class="math inline"><em>q</em>(<em>x</em><sub><em>t</em> − 1</sub>|<em>x</em><sub><em>t</em></sub>, <em>x</em><sub>0</sub>) </span>近似，即在训练阶段，通过已知的<span class="math inline"><em>x</em><sub>0</sub></span>构造出一个真实的后验分布作为学习目标。</p><p>近似原因：最终训练目标是训练模型<span class="math inline"><em>p</em><sub><em>θ</em></sub></span>，等价于最大化在给定模型下，观测数据<span class="math inline"><em>x</em><sub>0</sub></span> 的对数似然函数<span class="math inline"> log <em>p</em><sub><em>θ</em></sub>(<em>x</em><sub>0</sub>) </span>，<span class="math inline"> log <em>p</em><sub><em>θ</em></sub>(<em>x</em><sub>0</sub>) </span>的计算是困难的，可以通过变分下界（ELBO）来进行优化。<span class="math display">$$\log p_\theta(x_0)\geqL_{ELBO}=\mathbb{E}_{q(x_{1:T}|x_0)}\left[\log\frac{p_\theta(x_{0:T})}{q(x_{1:T}|x_0)}\right]$$</span> <span class="math display">$$L_{ELBO}=\underbrace{\mathbb{E}_q[\logp_\theta(x_0|x_1)]}_{\text{重构项}}-\underbrace{D_{KL}(q(x_T|x_0)||p(x_T))}_{\text{先验匹配项}}-\sum_{t=2}^T\underbrace{\mathbb{E}_q[D_{KL}(q(x_{t-1}|x_t,x_0)||p_\theta(x_{t-1}|x_t))]}_{\text{一致性项}}$$</span></p><ul><li>重构项<span class="math inline"><em>L</em><sub>0</sub></span>：从<span class="math inline"><em>x</em><sub><em>t</em></sub>恢复到<em>x</em><sub>0</sub></span>的过程，去噪过程的最后一步</li><li>先验匹配项<span class="math inline"><em>L</em><sub><em>T</em></sub></span>：<span class="math inline"><em>p</em>(<em>T</em>)</span>是一个标准的正态分布，由于前向过程<span class="math inline"><em>T</em></span>足够大，<span class="math inline"><em>q</em>(<em>x</em><sub><em>T</em></sub>|<em>x</em><sub>0</sub>)</span>也近似于一个标准正态分布，此项[[KL散度]]接近于0，可忽略不计，同时其不包含任何可训练参数</li><li>一致性项<span class="math inline"><em>L</em><sub><em>t</em> − 1</sub></span>：要求学习的反向学习过程<span class="math inline"><em>p</em><sub><em>θ</em></sub>(<em>x</em><sub><em>t</em> − 1</sub>|<em>x</em><sub><em>t</em></sub>)</span>与真实后验分布<span class="math inline"><em>q</em>(<em>x</em><sub><em>t</em></sub>|<em>x</em><sub><em>t</em> − 1</sub>, <em>x</em><sub>0</sub>)</span>尽可能匹配</li></ul><p><strong>一致性项<span class="math inline"><em>L</em><sub><em>t</em> − 1</sub></span>的简化</strong><span class="math display"><em>L</em><sub><em>t</em> − 1</sub> = 𝔼<sub><em>q</em></sub>[<em>D</em><sub><em>K</em><em>L</em></sub>(<em>q</em>(<em>x</em><sub><em>t</em> − 1</sub>|<em>x</em><sub><em>t</em></sub>, <em>x</em><sub>0</sub>)||<em>p</em><sub><em>θ</em></sub>(<em>x</em><sub><em>t</em> − 1</sub>|<em>x</em><sub><em>t</em></sub>))]</span>根据Bayes公式： <span class="math display">$$q(x_{t-1}|x_t,\,x_0)\,=\,\frac{q(x_t|x_{t-1},x_0)q(x_{t-1}|x_0)}{q(x_1|x_0)}=\frac{q(x_t|x_{t-1})q(x_{t-1}|x_0)}{q(x_1|x_0)}$$</span></p><p><span class="math inline"><em>q</em>(<em>x</em><sub><em>t</em></sub>|<em>x</em><sub><em>t</em> − 1</sub>, <em>x</em><sub>0</sub>)和<em>q</em>(<em>x</em><sub><em>t</em> − 1</sub>|<em>x</em><sub>0</sub>) </span>都是高斯分布，其乘积也是高斯分布。可以推导出<span class="math inline"><em>q</em>(<em>x</em><sub><em>t</em> − 1</sub>|<em>x</em><sub><em>t</em></sub>, <em>x</em><sub>0</sub>) </span>的均值和方差</p><p><span class="math display">$$\tilde{\mu}(x_t,x_0)=\frac{\sqrt{\alpha_{t-1}}\beta_{t}}{1-\alpha_{t}}x_{0}+\frac{\sqrt{1-\beta_{t}}(1-\alpha_{t-1})}{1-\alpha_{t}}x_{t}$$</span> <span class="math display">$$\tilde{\beta}_t=\frac{1-\alpha_{t-1}}{1-\alpha_t}\beta_t$$</span>在训练过程中均值和方差的参数均已知，故可以作为监督信号，指导神经网络的训练。通过最小化<span class="math inline"><em>p</em><sub><em>θ</em></sub> 和 <em>q</em> </span>之间的KL散度（或其他距离度量），可以有效训练模型</p><p>理论训练目标需要最小化<span class="math display"><em>D</em><sub><em>K</em><em>L</em></sub>(<em>q</em>(<em>x</em><sub><em>t</em> − 1</sub>|<em>x</em><sub><em>t</em></sub>, <em>x</em><sub>0</sub>) || <em>p</em><sub><em>θ</em></sub>(<em>x</em><sub><em>t</em> − 1</sub>|<em>x</em><sub><em>t</em></sub>) )</span><span class="math display"><em>p</em><sub><em>θ</em></sub>(<em>x</em><sub><em>t</em> − 1</sub>|<em>x</em><sub><em>t</em></sub>) = 𝒩(<em>x</em><sub><em>t</em> − 1</sub>; <em>μ</em><sub><em>θ</em></sub>(<em>x</em><sub><em>t</em></sub>, <em>t</em>), <em>σ</em><sub><em>t</em></sub><sup>2</sup><strong>I</strong>)</span><span class="math display"><em>q</em>(<em>x</em><sub><em>t</em> − 1</sub>|<em>x</em><sub><em>t</em></sub>, <em>x</em><sub>0</sub>) = 𝒩(<em>x</em><sub><em>t</em> − 1</sub>; <em>μ̃</em><sub><em>t</em></sub>(<em>x</em><sub><em>t</em></sub>, <em>x</em><sub>0</sub>), <em>β̃</em><sub><em>t</em></sub><strong>I</strong>)</span>两个高斯分布之间的[[KL散度]]会有一个解析解，由于方差项<span class="math inline"><em>σ</em><sub><em>t</em></sub><sup>2</sup></span>和<span class="math inline"><em>β̃</em><sub><em>t</em></sub></span>都是固定的（<em><span class="math inline"><em>σ</em><sub><em>t</em></sub><sup>2</sup></span>人为固定认为就是学习的最优值<span class="math inline"><em>β̃</em><sub><em>t</em></sub></span>，让网络学习均值而不是方差，是因为均值决定了生成内容，而方差决定了生成过程的随机性，生成目标是构建信号，故模型只需要学习均值，实验也证明了这一点</em>），不参与训练，故训练目标等价于最小化两个分布之间的L2距离： <span class="math display"><em>L</em><sub><em>t</em> − 1</sub> ∝ 𝔼<sub><em>q</em></sub>[||<em>μ̃</em><sub><em>t</em></sub>(<em>x</em><sub><em>t</em></sub>, <em>x</em><sub>0</sub>) − <em>μ</em><sub><em>θ</em></sub>(<em>x</em><sub><em>t</em></sub>, <em>t</em>)||<sup>2</sup>]</span>利用<span class="math inline">$x_{t}=\sqrt{\bar{\alpha}_{t}}x_{0}+\sqrt{1-\bar{\alpha}_{t}}\epsilon\,$</span>反解出<span class="math inline"><em>x</em><sub>0</sub></span>，代入<span class="math inline"><em>μ̃</em><sub><em>t</em></sub>(<em>x</em><sub><em>t</em></sub>, <em>x</em><sub>0</sub>)</span>，可以得到： <span class="math display">$$\begin{aligned}&amp;\tilde{\mu}_t(x_t,x_0)=\frac{1}{\sqrt{\alpha_t}}\left(x_t-\frac{\beta_t}{\sqrt{1-\bar{\alpha}_t}}\epsilon\right)\end{aligned}$$</span> 让<span class="math inline"><em>μ</em><sub><em>θ</em></sub>(<em>x</em><sub><em>t</em></sub>, <em>t</em>)</span>具有和<span class="math inline"><em>μ̃</em></span>相同的函数形式，即让神经网络去预测噪声<span class="math inline"><em>ϵ</em></span> <span class="math display">$$\mu_\theta(x_t,t)=\frac{1}{\sqrt{\alpha_t}}\left(x_t-\frac{\beta_t}{\sqrt{1-\bar{\alpha}_t}}\epsilon_\theta(x_t,t)\right)$$</span><span class="math inline"><em>θ</em>(<em>x</em><sub><em>t</em></sub>, <em>t</em>)</span>输入当前带噪数据数据<span class="math inline"><em>x</em><sub><em>t</em></sub></span>和时间步<span class="math inline"><em>t</em></span>，输出对噪声<span class="math inline"><em>ϵ</em></span>的预测 将<span class="math inline"><em>μ</em><sub><em>θ</em></sub></span>代入损失函数<span class="math display">$$\begin{aligned}&amp;L_{t-1}\propto\mathbb{E}_q\left[||\tilde{\mu}_t(x_t,x_0)-\mu_\theta(x_t,t)||^2\right]\\&amp;L_{t-1}\propto\mathbb{E}_q\left[||\frac{1}{\sqrt{\alpha_t}}\left(x_t-\frac{\beta_t}{\sqrt{1-\bar{\alpha}_t}}\epsilon\right)-\frac{1}{\sqrt{\alpha_t}}\left(x_t-\frac{\beta_t}{\sqrt{1-\bar{\alpha}_t}}\epsilon_\theta(x_t,t)\right)||^2\right]\end{aligned}$$</span> 得到： <span class="math display">$$L_{t-1}\propto\mathbb{E}_{x_0,\epsilon,t}\left[\frac{\beta_t^2}{2\sigma_t^2\alpha_t(1-\bar{\alpha}_t)}||\epsilon-\epsilon_\theta(x_t,t)||^2\right]$$</span> 由于前面的系数是正的常数（对于给定的t），在优化时可以被视为权重。DDPM论文发现，忽略这些权重，直接使用最简单的形式，效果也非常好优化KL散度等价于最小化噪声预测的MSE: <span class="math display">ℒ<sub><em>t</em></sub> = 𝔼<sub><em>x</em><sub>0</sub>, <em>ϵ</em>, <em>t</em></sub>[∥<em>ϵ</em> − <em>ϵ</em><sub><em>θ</em></sub>(<em>x</em><sub><em>t</em></sub>, <em>t</em>)∥<sup>2</sup>]</span>最终推导出的损失函数为，给定任意时刻<span class="math inline"> <em>t</em> </span>，带噪数据<span class="math inline"> <em>x</em><sub><em>t</em></sub> </span>，让神经网络<span class="math inline"> <em>ϵ</em><sub><em>θ</em></sub> </span>预测出当初用于生成<span class="math inline"> <em>x</em><sub><em>t</em></sub> </span>的原始噪声<span class="math inline"> <em>ϵ</em> </span>，并计算他们之间的均方误差用作反向传播</p><h2 id="算法">算法</h2><p><strong>整个过程是以去噪过程为核心，而非先加噪后去噪</strong></p><h3 id="训练算法">训练算法</h3><img src="/2025/07/06/Diffusion/0.png" class title="training"><p>训练过程的目标是得到一个能够预测噪声的神经网络<span class="math inline"><em>ϵ</em><sub><em>θ</em></sub>(<em>x</em><sub><em>t</em></sub>, <em>t</em>)</span>，即给定加噪后的的数据<span class="math inline"><em>x</em><sub><em>t</em></sub></span>和时间步<span class="math inline"><em>t</em></span> ，预测出原始加入的噪声<span class="math inline"><em>ϵ</em></span>，通过最小化真实噪声和预测噪声之间的均方误差来训练</p><ul><li>从数据集中随机采样一个真实的干净数据<span class="math inline"><em>q</em> ∼ <em>q</em>(<em>x</em><sub>0</sub>)</span></li><li>从时间步中<span class="math inline">{1, …, <em>T</em>}</span>中均匀采样一个时间步<span class="math inline"><em>t</em> ∼ Uniform({1, …, <em>T</em>})</span></li><li>从标准高斯分布中随机采样一个噪声<span class="math inline"><em>ϵ</em> ∼ 𝒩(0, I)</span></li><li>梯度下降计算损失，反向传播进行参数更新 <span class="math display">$$\nabla_\theta \left| \epsilon - \epsilon_\theta(\sqrt{\bar{\alpha}_t}x_0 + \sqrt{1 - \bar{\alpha}_t} \epsilon, t) \right|^2$$</span></li><li>直到模型收敛</li></ul><h3 id="采样算法">采样算法</h3><img src="/2025/07/06/Diffusion/2.png" class title="sampleing"><ul><li>从高斯噪声中随机采样一个数据</li><li>从 <span class="math inline"><em>T</em></span> 步开始反向去噪：<span class="math display">$$x_{t-1} = \frac{1}{\sqrt{\alpha_t}} \left( x_t - \frac{1 -\alpha_t}{\sqrt{1 - \bar{\alpha}_t}} \epsilon_\theta(x_t, t) \right) +\sigma_t z$$</span> 直到时间步 <span class="math inline"><em>t</em> = 1</span>结束推理，其中 <span class="math inline"><em>z</em></span>是一个新加入的随机噪声，保证生成过程的随机性，<span class="math inline"><em>σ</em><sub><em>t</em></sub></span>是一个反向过程的方差，通常设置为 <span class="math inline">$\sqrt{\beta_t}$</span></li></ul>]]></content>
    
    
    <categories>
      
      <category>生成式模型</category>
      
    </categories>
    
    
    <tags>
      
      <tag>生成式模型、</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>概率基础</title>
    <link href="/2025/07/06/%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/"/>
    <url>/2025/07/06/%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/</url>
    
    <content type="html"><![CDATA[<h2 id="条件概率联合概率边缘概率">条件概率、联合概率、边缘概率</h2><h3 id="条件概率">条件概率：</h3><p>在一个事件发生的情况下，另一个事件发生的概率 <span class="math display">$$P(B|A)=\frac{P(A,B)}{P(B)}$$</span> 链式法则（递推公式）： <span class="math display">$$P(X_1,X_2,X_3,\,\cdots\,,X_n)=P(X_i)\prod^{n}_{j,j\neq i}P(X_j)$$</span> ### 联合概率： 两个事件同时发生的概率，在数学上记为： <span class="math inline"><em>P</em>(<em>A</em>, <em>B</em>)</span></p><h3 id="边缘概率">边缘概率：</h3><p>多个事件中某个事件发生的概率，即定义在子集上的概率分布，可由全概率公式计算：<span class="math display"><em>P</em>(<em>a</em>) = ∑<sub><em>b</em></sub><em>P</em>(<em>a</em> , <em>b</em>)</span>全概率公式说明了边缘概率和联合概率的关系，而条件概率则说明了联合概率和条件概率之间的关系</p><h3 id="贝叶斯法则">贝叶斯法则：</h3><p>贝叶斯法则反映了先验概率和后验概率之间的关系 <span class="math display">$$P(A|B)=\frac{P(A)P(B|A)}{P(B)}$$</span></p>]]></content>
    
    
    <categories>
      
      <category>数学基础</category>
      
    </categories>
    
    
    <tags>
      
      <tag>概率、生成模型</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
