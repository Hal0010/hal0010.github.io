<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>flow matching</title>
    <link href="/2025/07/07/flow-matching/"/>
    <url>/2025/07/07/flow-matching/</url>
    
    <content type="html"><![CDATA[<h3 id="流匹配">流匹配</h3><p>流匹配是一种概率生成模型，通过学习一个常微分方程，实现从先验分布向目标分布的变换。微分方程可以定义为： <span class="math display">$$\frac{d}{dt}x_{t}=v_{t}(x_{t})$$</span></p><p>其中 <span class="math inline"><em>v</em><sub><em>t</em></sub>(<em>x</em><sub><em>t</em></sub>)</span>是时间和状态相关的<strong>速度场</strong>，这个速度场决定了数据如何从<span class="math inline"><em>p</em><sub>0</sub></span> 转换为 <span class="math inline"><em>p</em><sub>1</sub></span> 。如果从 <span class="math inline"><em>t</em> = 0</span>的一个噪声样本开始，沿着这个向量场积分到 <span class="math inline"><em>t</em> = 1</span>，就能得到一个数据样本 <span class="math inline"><em>x</em><sub>1</sub> ≈ <em>p</em><sub>1</sub></span>。</p><p>由于无法得知整个分布的演化，观察一个特定的数据点 <span class="math inline"><em>x</em><sub>1</sub> ∼ <em>p</em><sub>1</sub></span>是如何从噪声演变来的。可以定义一个任意从噪声点 <span class="math inline"><em>x</em><sub>0</sub> ∼ <em>p</em><sub>0</sub></span>到这个特定数据点 <span class="math inline"><em>x</em><sub>1</sub></span>的路径。这个路径只依赖于选定的 <span class="math inline"><em>x</em><sub>0</sub>和<em>x</em><sub>1</sub></span>，记条件概率路径为 <span class="math inline"><em>p</em><sub><em>t</em></sub>(<em>x</em>|<em>x</em><sub>0</sub>, <em>x</em><sub>1</sub>)</span>，定义了概率路径后就可以计算出对应的条件向量场<span class="math inline"><em>u</em><sub><em>t</em></sub>(<em>x</em>|<em>x</em><sub>0</sub>, <em>x</em><sub>1</sub>)</span>。</p><p><em>eg:矫正流</em> 使用线性插值定义概率路径： <span class="math display"><em>x</em><sub><em>t</em></sub> = (1 − <em>t</em>) ⋅ <em>x</em><sub>0</sub> + <em>x</em><sub>1</sub> ⋅ <em>t</em></span>对时间求导：得到目标向量场： <span class="math display"><em>u</em><sub><em>t</em></sub>(<em>x</em><sub><em>t</em></sub>|<em>x</em><sub>0</sub>, <em>x</em><sub>1</sub>) = <em>x</em><sub>1</sub> − <em>x</em><sub>0</sub></span>条件流匹配（CFM）的损失可以定义为： <span class="math display">ℒ<sub>CFM</sub>(<em>θ</em>) = 𝔼<sub><em>t</em>, <strong>x</strong><sub>0</sub>, <strong>x</strong><sub>1</sub></sub>[||<strong>v</strong><sub><em>θ</em></sub>(<strong>x</strong><sub><em>t</em></sub>, <em>t</em>) − <strong>u</strong><sub><em>t</em></sub>(<strong>x</strong><sub><em>t</em></sub>|<strong>x</strong><sub>0</sub>, <strong>x</strong><sub>1</sub>)||<sup>2</sup>]</span>其中，<span class="math inline">𝔼</span> 表示对时间 <span class="math inline"><em>t</em> ∼ <em>U</em>(0, 1)</span> ，噪声 <span class="math inline"><em>x</em><sub>0</sub> ∼ <em>p</em><sub>0</sub></span>和目标分布 <span class="math inline"><em>x</em><sub>1</sub> ∼ <em>p</em><sub>1</sub></span>进行采样求期望，实际计算形式为： <span class="math display">ℒ<sub>RectifiedFlow</sub>(<em>θ</em>) = 𝔼<sub><em>t</em>, <strong>x</strong><sub>0</sub>, <strong>x</strong><sub>1</sub></sub>[||<strong>v</strong><sub><em>θ</em></sub>((1 − <em>t</em>)<strong>x</strong><sub>0</sub> + <em>t</em><strong>x</strong><sub>1</sub>, <em>t</em>) − (<strong>x</strong><sub>1</sub> − <strong>x</strong><sub>0</sub>)||<sup>2</sup>]</span>数学上可以严格证明： <strong>最小化条件匹配流损失</strong> <span class="math inline">ℒ<sub>CFM</sub></span> 等价于最小化边缘匹配流损失<span class="math inline">ℒ<sub>marginal</sub></span> 故： <span class="math display">∇<sub><em>θ</em></sub>ℒ<sub>CFM</sub>(<em>θ</em>) = ∇<sub><em>θ</em></sub>ℒ<sub>marginal</sub>(<em>θ</em>)</span>通过让模型学习 <span class="math inline"><em>u</em><sub><em>t</em></sub>(<em>x</em><sub><em>t</em></sub>|<em>x</em><sub>0</sub>, <em>x</em><sub>1</sub>)</span>，即可得到目标向量场。</p>]]></content>
    
    
    <categories>
      
      <category>生成式模型</category>
      
    </categories>
    
    
    <tags>
      
      <tag>生成式、深度学习、概率模型</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Diffusion</title>
    <link href="/2025/07/06/Diffusion/"/>
    <url>/2025/07/06/Diffusion/</url>
    
    <content type="html"><![CDATA[<h1 id="ddpm">DDPM</h1><ol type="1"><li>加噪过程一步到位，去噪过程严格按照一步一步去噪，实现对噪声的预测，由于去噪过程需要一步一步进行，所以每次都要调用神经网络，导致生成较慢，所以考虑DDIM等加速生成的算法框架</li><li>加噪过程：前向过程被认为是马尔可夫过程，需要将数据分布逐步转换为噪声分布<span class="math display">$$q(x_0,\ x_1,\ \dots\ ,x_T)=q(x_0)\prod^{T}_{i}q(x_t|x_{t-1})$$</span> 输入数据<span class="math inline"><em>x</em></span>，噪声水平<span class="math inline"><em>t</em></span>，真实噪声<span class="math inline"><em>ϵ</em></span>。</li><li>逆向过程：从噪声分布逆向还原数据分布，学习目标： <span class="math display">$$p_{\theta}(x_0,x_1,\dots,x_T)=p(x_T)\prod^{T}_{i}p_{\theta}(x_{t-1}|x_t)$$</span></li></ol><h2 id="数学推导">数学推导</h2><p>扩散模型的训练本质上是变分推断，通过最大化证据下界（ELBO）来近似真实的后验分布<span class="math inline"> <em>p</em>(<em>x</em><sub><em>t</em> − 1</sub>|<em>x</em><sub><em>t</em></sub>)</span>，<span class="math inline"><em>q</em>(<em>x</em><sub><em>t</em> − 1</sub>|<em>x</em><sub><em>t</em></sub>, <em>x</em><sub>0</sub>) </span>是真实后验的一个变分近似，而<span class="math inline"> <em>p</em><sub><em>θ</em></sub>(<em>x</em><sub><em>t</em> − 1</sub>|<em>x</em><sub><em>t</em></sub>)</span>是模型学习的分布</p><p>原始数据点分布<span class="math inline"><em>x</em><sub>0</sub> ∼ <em>q</em>(<em>x</em><sub>0</sub>)</span>，构造马尔可夫加噪过程：<span class="math display">$$q(x_{1:T}|x_0)=\prod^{T}_{i}q(x_t|x_{t-1})$$</span> 状态转移核服从高斯条件分布： <span class="math display">$$q(x_t|x_{t-1})=\mathcal{N}(x_t\ ; \, \sqrt{1-\beta_t}x_{t-1}\ ,\ \beta\mathbf{I})\ ,\ \beta_t \in (0,1)_{t=1}^T$$</span> <span class="math inline"><em>β</em><sub><em>t</em></sub></span>为预定的噪声方差系数，由时间<span class="math inline"><strong>1</strong> <strong>∼</strong> <strong>T</strong></span>逐渐递增，可以是线性、指数等，<span class="math inline">$\sqrt{1-\beta_t}x_{t-1}\,$</span>为高斯分布的均值<em>经推导</em>： <span class="math display">$$\begin{equation}x_t = \sqrt{\alpha_t} x_0 + \sqrt{1 - {\alpha}_t} \, \epsilon, \quad\epsilon \sim \mathcal{N}(0,1)\end{equation}$$</span> 任意步的数值都可以从<span class="math inline"><em>x</em><sub>0</sub></span>导出，其中<span class="math inline">$\,\alpha_t=\prod^{t}_{s=1}(1-\beta_s)\,$</span></p><p>逆向过程也是一个马尔可夫链，通过学习条件概率<span class="math inline"> <em>p</em><sub><em>θ</em></sub>(<em>x</em><sub><em>t</em> − 1</sub>|<em>x</em><sub><em>t</em></sub>) </span>逐步恢复<span class="math inline"><em>x</em><sub>0</sub></span>，引入神经网络<span class="math inline"><em>p</em><sub><em>θ</em></sub></span>来近似 <span class="math display"> <em>p</em><sub><em>θ</em></sub>(<em>x</em><sub><em>t</em> − 1</sub>|<em>x</em><sub><em>t</em></sub>)  = 𝒩(<em>x</em><sub><em>t</em> − 1</sub>; <em>μ</em><sub><em>θ</em></sub>(<em>x</em><sub><em>t</em></sub>, <em>t</em>), ∑<sub><em>θ</em></sub>(<em>x</em><sub><em>t</em></sub>, <em>t</em>))</span>其中，<span class="math inline"><em>μ</em><sub><em>θ</em></sub></span>和 <span class="math inline">∑<sub><em>θ</em></sub></span>是神经网络需要学习的均值和方差，通常<span class="math inline"> <em>p</em><sub><em>θ</em></sub>(<em>x</em><sub><em>t</em> − 1</sub>|<em>x</em><sub><em>t</em></sub>) </span>难以训练，可以使用<span class="math inline"><em>q</em>(<em>x</em><sub><em>t</em> − 1</sub>|<em>x</em><sub><em>t</em></sub>) </span>近似，而<span class="math inline"><em>q</em>(<em>x</em><sub><em>t</em> − 1</sub>|<em>x</em><sub><em>t</em></sub>) </span>难以直接估计，无法直接求出闭式解，所以使用可计算的真实后验<span class="math inline"><em>q</em>(<em>x</em><sub><em>t</em> − 1</sub>|<em>x</em><sub><em>t</em></sub>, <em>x</em><sub>0</sub>) </span>近似，即在训练阶段，通过已知的<span class="math inline"><em>x</em><sub>0</sub></span>构造出一个真实的后验分布作为学习目标。</p><p>近似原因：最终训练目标是训练模型<span class="math inline"><em>p</em><sub><em>θ</em></sub></span>，等价于最大化在给定模型下，观测数据<span class="math inline"><em>x</em><sub>0</sub></span> 的对数似然函数<span class="math inline"> log <em>p</em><sub><em>θ</em></sub>(<em>x</em><sub>0</sub>) </span>，<span class="math inline"> log <em>p</em><sub><em>θ</em></sub>(<em>x</em><sub>0</sub>) </span>的计算是困难的，可以通过变分下界（ELBO）来进行优化。<span class="math display">$$\log p_\theta(x_0)\geqL_{ELBO}=\mathbb{E}_{q(x_{1:T}|x_0)}\left[\log\frac{p_\theta(x_{0:T})}{q(x_{1:T}|x_0)}\right]$$</span> <span class="math display">$$L_{ELBO}=\underbrace{\mathbb{E}_q[\logp_\theta(x_0|x_1)]}_{\text{重构项}}-\underbrace{D_{KL}(q(x_T|x_0)||p(x_T))}_{\text{先验匹配项}}-\sum_{t=2}^T\underbrace{\mathbb{E}_q[D_{KL}(q(x_{t-1}|x_t,x_0)||p_\theta(x_{t-1}|x_t))]}_{\text{一致性项}}$$</span></p><ul><li>重构项<span class="math inline"><em>L</em><sub>0</sub></span>：从<span class="math inline"><em>x</em><sub><em>t</em></sub>恢复到<em>x</em><sub>0</sub></span>的过程，去噪过程的最后一步</li><li>先验匹配项<span class="math inline"><em>L</em><sub><em>T</em></sub></span>：<span class="math inline"><em>p</em>(<em>T</em>)</span>是一个标准的正态分布，由于前向过程<span class="math inline"><em>T</em></span>足够大，<span class="math inline"><em>q</em>(<em>x</em><sub><em>T</em></sub>|<em>x</em><sub>0</sub>)</span>也近似于一个标准正态分布，此项[[KL散度]]接近于0，可忽略不计，同时其不包含任何可训练参数</li><li>一致性项<span class="math inline"><em>L</em><sub><em>t</em> − 1</sub></span>：要求学习的反向学习过程<span class="math inline"><em>p</em><sub><em>θ</em></sub>(<em>x</em><sub><em>t</em> − 1</sub>|<em>x</em><sub><em>t</em></sub>)</span>与真实后验分布<span class="math inline"><em>q</em>(<em>x</em><sub><em>t</em></sub>|<em>x</em><sub><em>t</em> − 1</sub>, <em>x</em><sub>0</sub>)</span>尽可能匹配</li></ul><p><strong>一致性项<span class="math inline"><em>L</em><sub><em>t</em> − 1</sub></span>的简化</strong><span class="math display"><em>L</em><sub><em>t</em> − 1</sub> = 𝔼<sub><em>q</em></sub>[<em>D</em><sub><em>K</em><em>L</em></sub>(<em>q</em>(<em>x</em><sub><em>t</em> − 1</sub>|<em>x</em><sub><em>t</em></sub>, <em>x</em><sub>0</sub>)||<em>p</em><sub><em>θ</em></sub>(<em>x</em><sub><em>t</em> − 1</sub>|<em>x</em><sub><em>t</em></sub>))]</span>根据Bayes公式： <span class="math display">$$q(x_{t-1}|x_t,\,x_0)\,=\,\frac{q(x_t|x_{t-1},x_0)q(x_{t-1}|x_0)}{q(x_1|x_0)}=\frac{q(x_t|x_{t-1})q(x_{t-1}|x_0)}{q(x_1|x_0)}$$</span></p><p><span class="math inline"><em>q</em>(<em>x</em><sub><em>t</em></sub>|<em>x</em><sub><em>t</em> − 1</sub>, <em>x</em><sub>0</sub>)和<em>q</em>(<em>x</em><sub><em>t</em> − 1</sub>|<em>x</em><sub>0</sub>) </span>都是高斯分布，其乘积也是高斯分布。可以推导出<span class="math inline"><em>q</em>(<em>x</em><sub><em>t</em> − 1</sub>|<em>x</em><sub><em>t</em></sub>, <em>x</em><sub>0</sub>) </span>的均值和方差</p><p><span class="math display">$$\tilde{\mu}(x_t,x_0)=\frac{\sqrt{\alpha_{t-1}}\beta_{t}}{1-\alpha_{t}}x_{0}+\frac{\sqrt{1-\beta_{t}}(1-\alpha_{t-1})}{1-\alpha_{t}}x_{t}$$</span> <span class="math display">$$\tilde{\beta}_t=\frac{1-\alpha_{t-1}}{1-\alpha_t}\beta_t$$</span>在训练过程中均值和方差的参数均已知，故可以作为监督信号，指导神经网络的训练。通过最小化<span class="math inline"><em>p</em><sub><em>θ</em></sub> 和 <em>q</em> </span>之间的KL散度（或其他距离度量），可以有效训练模型</p><p>理论训练目标需要最小化<span class="math display"><em>D</em><sub><em>K</em><em>L</em></sub>(<em>q</em>(<em>x</em><sub><em>t</em> − 1</sub>|<em>x</em><sub><em>t</em></sub>, <em>x</em><sub>0</sub>) || <em>p</em><sub><em>θ</em></sub>(<em>x</em><sub><em>t</em> − 1</sub>|<em>x</em><sub><em>t</em></sub>) )</span><span class="math display"><em>p</em><sub><em>θ</em></sub>(<em>x</em><sub><em>t</em> − 1</sub>|<em>x</em><sub><em>t</em></sub>) = 𝒩(<em>x</em><sub><em>t</em> − 1</sub>; <em>μ</em><sub><em>θ</em></sub>(<em>x</em><sub><em>t</em></sub>, <em>t</em>), <em>σ</em><sub><em>t</em></sub><sup>2</sup><strong>I</strong>)</span><span class="math display"><em>q</em>(<em>x</em><sub><em>t</em> − 1</sub>|<em>x</em><sub><em>t</em></sub>, <em>x</em><sub>0</sub>) = 𝒩(<em>x</em><sub><em>t</em> − 1</sub>; <em>μ̃</em><sub><em>t</em></sub>(<em>x</em><sub><em>t</em></sub>, <em>x</em><sub>0</sub>), <em>β̃</em><sub><em>t</em></sub><strong>I</strong>)</span>两个高斯分布之间的[[KL散度]]会有一个解析解，由于方差项<span class="math inline"><em>σ</em><sub><em>t</em></sub><sup>2</sup></span>和<span class="math inline"><em>β̃</em><sub><em>t</em></sub></span>都是固定的（<em><span class="math inline"><em>σ</em><sub><em>t</em></sub><sup>2</sup></span>人为固定认为就是学习的最优值<span class="math inline"><em>β̃</em><sub><em>t</em></sub></span>，让网络学习均值而不是方差，是因为均值决定了生成内容，而方差决定了生成过程的随机性，生成目标是构建信号，故模型只需要学习均值，实验也证明了这一点</em>），不参与训练，故训练目标等价于最小化两个分布之间的L2距离： <span class="math display"><em>L</em><sub><em>t</em> − 1</sub> ∝ 𝔼<sub><em>q</em></sub>[||<em>μ̃</em><sub><em>t</em></sub>(<em>x</em><sub><em>t</em></sub>, <em>x</em><sub>0</sub>) − <em>μ</em><sub><em>θ</em></sub>(<em>x</em><sub><em>t</em></sub>, <em>t</em>)||<sup>2</sup>]</span>利用<span class="math inline">$x_{t}=\sqrt{\bar{\alpha}_{t}}x_{0}+\sqrt{1-\bar{\alpha}_{t}}\epsilon\,$</span>反解出<span class="math inline"><em>x</em><sub>0</sub></span>，代入<span class="math inline"><em>μ̃</em><sub><em>t</em></sub>(<em>x</em><sub><em>t</em></sub>, <em>x</em><sub>0</sub>)</span>，可以得到： <span class="math display">$$\begin{aligned}&amp;\tilde{\mu}_t(x_t,x_0)=\frac{1}{\sqrt{\alpha_t}}\left(x_t-\frac{\beta_t}{\sqrt{1-\bar{\alpha}_t}}\epsilon\right)\end{aligned}$$</span> 让<span class="math inline"><em>μ</em><sub><em>θ</em></sub>(<em>x</em><sub><em>t</em></sub>, <em>t</em>)</span>具有和<span class="math inline"><em>μ̃</em></span>相同的函数形式，即让神经网络去预测噪声<span class="math inline"><em>ϵ</em></span> <span class="math display">$$\mu_\theta(x_t,t)=\frac{1}{\sqrt{\alpha_t}}\left(x_t-\frac{\beta_t}{\sqrt{1-\bar{\alpha}_t}}\epsilon_\theta(x_t,t)\right)$$</span><span class="math inline"><em>θ</em>(<em>x</em><sub><em>t</em></sub>, <em>t</em>)</span>输入当前带噪数据数据<span class="math inline"><em>x</em><sub><em>t</em></sub></span>和时间步<span class="math inline"><em>t</em></span>，输出对噪声<span class="math inline"><em>ϵ</em></span>的预测 将<span class="math inline"><em>μ</em><sub><em>θ</em></sub></span>代入损失函数<span class="math display">$$\begin{aligned}&amp;L_{t-1}\propto\mathbb{E}_q\left[||\tilde{\mu}_t(x_t,x_0)-\mu_\theta(x_t,t)||^2\right]\\&amp;L_{t-1}\propto\mathbb{E}_q\left[||\frac{1}{\sqrt{\alpha_t}}\left(x_t-\frac{\beta_t}{\sqrt{1-\bar{\alpha}_t}}\epsilon\right)-\frac{1}{\sqrt{\alpha_t}}\left(x_t-\frac{\beta_t}{\sqrt{1-\bar{\alpha}_t}}\epsilon_\theta(x_t,t)\right)||^2\right]\end{aligned}$$</span> 得到： <span class="math display">$$L_{t-1}\propto\mathbb{E}_{x_0,\epsilon,t}\left[\frac{\beta_t^2}{2\sigma_t^2\alpha_t(1-\bar{\alpha}_t)}||\epsilon-\epsilon_\theta(x_t,t)||^2\right]$$</span> 由于前面的系数是正的常数（对于给定的t），在优化时可以被视为权重。DDPM论文发现，忽略这些权重，直接使用最简单的形式，效果也非常好优化KL散度等价于最小化噪声预测的MSE: <span class="math display">ℒ<sub><em>t</em></sub> = 𝔼<sub><em>x</em><sub>0</sub>, <em>ϵ</em>, <em>t</em></sub>[∥<em>ϵ</em> − <em>ϵ</em><sub><em>θ</em></sub>(<em>x</em><sub><em>t</em></sub>, <em>t</em>)∥<sup>2</sup>]</span>最终推导出的损失函数为，给定任意时刻<span class="math inline"> <em>t</em> </span>，带噪数据<span class="math inline"> <em>x</em><sub><em>t</em></sub> </span>，让神经网络<span class="math inline"> <em>ϵ</em><sub><em>θ</em></sub> </span>预测出当初用于生成<span class="math inline"> <em>x</em><sub><em>t</em></sub> </span>的原始噪声<span class="math inline"> <em>ϵ</em> </span>，并计算他们之间的均方误差用作反向传播</p><h2 id="算法">算法</h2><p><strong>整个过程是以去噪过程为核心，而非先加噪后去噪</strong></p><h3 id="训练算法">训练算法</h3><img src="/2025/07/06/Diffusion/0.png" class title="training"><p>训练过程的目标是得到一个能够预测噪声的神经网络<span class="math inline"><em>ϵ</em><sub><em>θ</em></sub>(<em>x</em><sub><em>t</em></sub>, <em>t</em>)</span>，即给定加噪后的的数据<span class="math inline"><em>x</em><sub><em>t</em></sub></span>和时间步<span class="math inline"><em>t</em></span> ，预测出原始加入的噪声<span class="math inline"><em>ϵ</em></span>，通过最小化真实噪声和预测噪声之间的均方误差来训练</p><ul><li>从数据集中随机采样一个真实的干净数据<span class="math inline"><em>q</em> ∼ <em>q</em>(<em>x</em><sub>0</sub>)</span></li><li>从时间步中<span class="math inline">{1, …, <em>T</em>}</span>中均匀采样一个时间步<span class="math inline"><em>t</em> ∼ Uniform({1, …, <em>T</em>})</span></li><li>从标准高斯分布中随机采样一个噪声<span class="math inline"><em>ϵ</em> ∼ 𝒩(0, I)</span></li><li>梯度下降计算损失，反向传播进行参数更新 <span class="math display">$$\nabla_\theta \left| \epsilon - \epsilon_\theta(\sqrt{\bar{\alpha}_t}x_0 + \sqrt{1 - \bar{\alpha}_t} \epsilon, t) \right|^2$$</span></li><li>直到模型收敛</li></ul><h3 id="采样算法">采样算法</h3><img src="/2025/07/06/Diffusion/2.png" class title="sampleing"><ul><li>从高斯噪声中随机采样一个数据</li><li>从 <span class="math inline"><em>T</em></span> 步开始反向去噪：<span class="math display">$$x_{t-1} = \frac{1}{\sqrt{\alpha_t}} \left( x_t - \frac{1 -\alpha_t}{\sqrt{1 - \bar{\alpha}_t}} \epsilon_\theta(x_t, t) \right) +\sigma_t z$$</span> 直到时间步 <span class="math inline"><em>t</em> = 1</span>结束推理，其中 <span class="math inline"><em>z</em></span>是一个新加入的随机噪声，保证生成过程的随机性，<span class="math inline"><em>σ</em><sub><em>t</em></sub></span>是一个反向过程的方差，通常设置为 <span class="math inline">$\sqrt{\beta_t}$</span></li></ul>]]></content>
    
    
    <categories>
      
      <category>生成式模型</category>
      
    </categories>
    
    
    <tags>
      
      <tag>生成式模型、</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>概率基础</title>
    <link href="/2025/07/06/%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/"/>
    <url>/2025/07/06/%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/</url>
    
    <content type="html"><![CDATA[<h2 id="条件概率联合概率边缘概率">条件概率、联合概率、边缘概率</h2><h3 id="条件概率">条件概率：</h3><p>在一个事件发生的情况下，另一个事件发生的概率 <span class="math display">$$P(B|A)=\frac{P(A,B)}{P(B)}$$</span> 链式法则（递推公式）： <span class="math display">$$P(X_1,X_2,X_3,\,\cdots\,,X_n)=P(X_i)\prod^{n}_{j,j\neq i}P(X_j)$$</span> ### 联合概率： 两个事件同时发生的概率，在数学上记为： <span class="math inline"><em>P</em>(<em>A</em>, <em>B</em>)</span></p><h3 id="边缘概率">边缘概率：</h3><p>多个事件中某个事件发生的概率，即定义在子集上的概率分布，可由全概率公式计算：<span class="math display"><em>P</em>(<em>a</em>) = ∑<sub><em>b</em></sub><em>P</em>(<em>a</em> , <em>b</em>)</span>全概率公式说明了边缘概率和联合概率的关系，而条件概率则说明了联合概率和条件概率之间的关系</p><h3 id="贝叶斯法则">贝叶斯法则：</h3><p>贝叶斯法则反映了先验概率和后验概率之间的关系 <span class="math display">$$P(A|B)=\frac{P(A)P(B|A)}{P(B)}$$</span></p>]]></content>
    
    
    <categories>
      
      <category>数学基础</category>
      
    </categories>
    
    
    <tags>
      
      <tag>概率、生成模型</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
